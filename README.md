# ğŸ¾ Footprint Classification Project

A deep learning project that classifies **animal** and **dinosaur footprints** using Convolutional Neural Networks (CNNs) and Transfer Learning with TensorFlow/Keras.

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Project Structure](#project-structure)
- [Datasets](#datasets)
- [Models](#models)
- [Installation](#installation)
- [Usage](#usage)
- [Results](#results)
- [Acknowledgements & Data Sources](#acknowledgements--data-sources)
- [License](#license)

---

## Overview

This project explores image classification techniques on two distinct footprint datasets:

1. **Animal Footprint Classification** â€” Classifying footprints of domestic cats, domestic dogs, and European badgers using both a custom CNN and MobileNetV2 transfer learning.
2. **Dinosaur Footprint Classification** â€” Classifying dinosaur track silhouettes into groups (Theropoda, Ornithopoda, Stegosauria) using data from the DinoTracker research project.

---

## Project Structure

```
Footprint_Classification_Project/
â”‚
â”œâ”€â”€ data/                              # Animal footprint images
â”‚   â”œâ”€â”€ train/                         # Training set (163 images)
â”‚   â”œâ”€â”€ valid/                         # Validation set (31 images)
â”‚   â””â”€â”€ test/                          # Test set (61 images)
â”‚       â”œâ”€â”€ domestic_cat/
â”‚       â”œâ”€â”€ domestic_dog/
â”‚       â””â”€â”€ european_badger/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ CNN/                           # Custom CNN approach
â”‚   â”‚   â”œâ”€â”€ 01_setup_and_explore.ipynb
â”‚   â”‚   â”œâ”€â”€ 02_data_loading.ipynb
â”‚   â”‚   â”œâ”€â”€ 03_build_model.ipynb
â”‚   â”‚   â”œâ”€â”€ 04_train_model.ipynb
â”‚   â”‚   â”œâ”€â”€ 05_evaluate_model.ipynb
â”‚   â”‚   â””â”€â”€ complete_pipeline.ipynb    # End-to-end CNN pipeline
â”‚   â””â”€â”€ TransferLearning/             # MobileNetV2 approach
â”‚       â”œâ”€â”€ 01_understanding_transfer_learning.ipynb
â”‚       â”œâ”€â”€ 02_data_setup.ipynb
â”‚       â”œâ”€â”€ 03_build_model.ipynb
â”‚       â”œâ”€â”€ 04_train_model.ipynb
â”‚       â”œâ”€â”€ 05_evaluate_model.ipynb
â”‚       â””â”€â”€ complete_pipeline.ipynb    # End-to-end TL pipeline
â”‚
â”œâ”€â”€ models/                            # Saved models & visualizations
â”‚
â”œâ”€â”€ Dino Footprint/                    # Dinosaur footprint sub-project
â”‚   â”œâ”€â”€ data/                          # DinoTracker dataset files
â”‚   â”‚   â”œâ”€â”€ images_compressed.npz
â”‚   â”‚   â”œâ”€â”€ names.npy
â”‚   â”‚   â””â”€â”€ tracks.xlsx
â”‚   â”œâ”€â”€ models/                        # Dino classifier outputs
â”‚   â”œâ”€â”€ dino_pipeline.ipynb            # Complete dino classifier
â”‚   â””â”€â”€ inspect_data.py               # Data inspection utility
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## Datasets

### Animal Footprints

- **Classes**: Domestic Cat, Domestic Dog, European Badger
- **Total images**: 255 (163 train / 31 validation / 61 test)
- **Format**: RGB photographs organized in class-based folders
- **Source**: Derived from the [AnimalClue YOLO Detection](https://huggingface.co/spaces/risashinoda/animalclue_yolo_det) space on Hugging Face by [risashinoda](https://huggingface.co/risashinoda)

### Dinosaur Footprints

- **Classes**: Theropoda (967), Ornithopoda (661), Stegosauria (52)
- **Total images**: 1,680 matched samples (from 1,976 track entries)
- **Format**: Binary silhouette images stored as compressed NumPy arrays (`.npz`) with metadata in Excel (`.xlsx`)
- **Source**: [DinoTracker](https://github.com/gregh83/DinoTracker) by Gregor Hartmann et al.

---

## Models

| Model | Architecture | Dataset | Test Accuracy | Model Size |
|---|---|---|---|---|
| Custom CNN | Conv2D from scratch | Animal Footprints | 49.0% | ~128 MB |
| Transfer Learning | MobileNetV2 (fine-tuned) | Animal Footprints | 60.7% | ~20 MB |
| Dino CNN | Conv2D from scratch | Dinosaur Footprints | ~69% (train) | ~58 MB |

Each notebook pipeline is structured in 5 progressive steps:
1. **Setup & Exploration** â€” Environment config, data overview
2. **Data Loading** â€” Image generators, augmentation, preprocessing
3. **Model Building** â€” Architecture definition
4. **Training** â€” Training loop with callbacks (early stopping, checkpointing)
5. **Evaluation** â€” Confusion matrix, sample predictions, metrics

---

## Installation

### Prerequisites

- Python 3.10+
- pip

### Setup

```bash
# Clone the repository
git clone <repository-url>
cd Footprint_Classification_Project

# Create a virtual environment (recommended)
python -m venv venv
source venv/bin/activate        # Linux/macOS
venv\Scripts\activate           # Windows

# Install dependencies
pip install -r requirements.txt
```

---

## Usage

### Animal Footprint Classification

Run the complete pipelines using Jupyter:

```bash
jupyter notebook
```

Then open either:
- `notebooks/CNN/complete_pipeline.ipynb` â€” Custom CNN approach
- `notebooks/TransferLearning/complete_pipeline.ipynb` â€” MobileNetV2 approach

Or follow the step-by-step notebooks (`01` through `05`) for a detailed walkthrough.

### Dinosaur Footprint Classification

```bash
# Inspect the raw dino data
python "Dino Footprint/inspect_data.py"

# Run the full dino pipeline
jupyter notebook "Dino Footprint/dino_pipeline.ipynb"
```

---

## Results

### Animal Footprints â€” CNN vs Transfer Learning

| Metric | Custom CNN | Transfer Learning (MobileNetV2) |
|---|---|---|
| Test Accuracy | 49.0% | **60.7%** |
| Training Epochs | 14 (early stop) | 10 + 5 (two-phase) |
| Best Validation Acc | ~55% | ~68% |

### Dinosaur Footprints

| Metric | Value |
|---|---|
| Best Validation Accuracy | ~77% (epoch 1) |
| Final Training Accuracy | ~63% |
| Classes Predicted | 2 of 3 (Stegosauria never predicted) |

> **Note**: Performance is limited by the small and imbalanced datasets. Future work could improve results through larger datasets, class weighting, advanced augmentation, and stronger backbone architectures.

---

## Acknowledgements & Data Sources

### ğŸ¾ Animal Footprint Data â€” AnimalClue

The animal footprint images used in this project are derived from the **AnimalClue** dataset, a large-scale benchmark for recognizing animals by their indirect traces (ICCV 2025 Highlight).

- **Project Page**: [AnimalClue: Recognizing Animals by their Traces](https://dahlian00.github.io/AnimalCluePage/)
- **Paper**: [arXiv:2507.20240](https://arxiv.org/abs/2507.20240)
- **Demo**: [AnimalClue YOLO Detection](https://huggingface.co/spaces/risashinoda/animalclue_yolo_det) on Hugging Face
- **Dataset**: [risashinoda/footprint_yolo](https://huggingface.co/datasets/risashinoda/footprint_yolo) on Hugging Face
- **Code**: [dahlian00/AnimalClue](https://github.com/dahlian00/AnimalClue) on GitHub
- **Authors**: Risa Shinoda, Nakamasa Inoue, Iro Laina, Christian Rupprecht, Hirokatsu Kataoka
- **Affiliations**: University of Osaka, Kyoto University, Tokyo Institute of Technology, AIST, University of Oxford (VGG)

As requested by the authors, we cite the following paper:

```bibtex
@article{shinoda2025animalcluerecognizinganimalstraces,
  title={AnimalClue: Recognizing Animals by their Traces},
  author={Risa Shinoda and Nakamasa Inoue and Iro Laina and Christian Rupprecht and Hirokatsu Kataoka},
  year={2025},
  eprint={2507.20240},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2507.20240},
}
```

We used footprint images from 3 species (domestic cat, domestic dog, European badger) from their footprint dataset. We thank the AnimalClue team for making their dataset and research openly available.

---

### ğŸ¦• Dinosaur Footprint Data â€” DinoTracker

The dinosaur footprint data used in this project comes from the **DinoTracker** project. As requested by the authors, we cite the following paper:

> G. Hartmann, T. Blakesley, P.E. dePolo, & S.L. Brusatte, *Identifying variation in dinosaur footprints and classifying problematic specimens via unbiased unsupervised machine learning*, Proc. Natl. Acad. Sci. U.S.A. **123** (5) e2527222122, [https://doi.org/10.1073/pnas.2527222122](https://doi.org/10.1073/pnas.2527222122) (2026).

- **Source**: [DinoTracker â€” GitHub Repository](https://github.com/gregh83/DinoTracker)
- **Authors**: Gregor Hartmann, T. Blakesley, P.E. dePolo, & S.L. Brusatte
- **License**: GPL-3.0
- **Description**: An app for dinosaur footprint analysis via disentangled variational autoencoder. The project has been covered by international media including The Guardian, BBC Newsround, Reuters, and The Conversation.
- **Contact**: ğŸ“§ [gregor.hartmann@helmholtz-berlin.de](mailto:gregor.hartmann@helmholtz-berlin.de)

We are grateful to the DinoTracker team for making their dataset and research openly available.

---

## Technologies Used

- [TensorFlow](https://www.tensorflow.org/) / [Keras](https://keras.io/) â€” Deep learning framework
- [MobileNetV2](https://arxiv.org/abs/1801.04381) â€” Pre-trained backbone for transfer learning
- [NumPy](https://numpy.org/) â€” Numerical computing
- [Matplotlib](https://matplotlib.org/) / [Seaborn](https://seaborn.pydata.org/) â€” Visualization
- [scikit-learn](https://scikit-learn.org/) â€” Evaluation metrics
- [openpyxl](https://openpyxl.readthedocs.io/) â€” Excel file parsing
- [Jupyter Notebook](https://jupyter.org/) â€” Interactive development

---

## License

This project is provided for educational and research purposes. Please note:
- The **AnimalClue** dataset images are linked to individual observation IDs, and usage of each image must comply with the license of the corresponding observation. The project website is licensed under [CC BY-SA 4.0](http://creativecommons.org/licenses/by-sa/4.0/).
- The **DinoTracker** data and code are distributed under the [GPL-3.0 License](https://github.com/gregh83/DinoTracker/blob/main/LICENSE).
- Please refer to the individual data source licenses for any restrictions on redistribution or commercial use.
